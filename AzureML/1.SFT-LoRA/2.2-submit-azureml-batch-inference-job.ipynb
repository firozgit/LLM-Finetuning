{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740f4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.entities import (\n",
    "    Environment, \n",
    "    BatchEndpoint, \n",
    "    BatchDeployment, \n",
    "    CodeConfiguration,\n",
    "    BatchRetrySettings,\n",
    "    ResourceConfiguration,\n",
    "    AmlCompute\n",
    ")\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d003f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to log in to Azure\n",
    "#!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3568e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global logging level\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# Specifically reduce Azure-related logging in this notebook\n",
    "logging.getLogger(\"azure\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17568a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Azure ML workspace configuration from config.yml\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Azure ML workspace configuration\n",
    "subscription_id = config[\"subscription_id\"]\n",
    "resource_group = config[\"resource_group\"]\n",
    "workspace_name = config[\"workspace_name\"]\n",
    "\n",
    "# Fintuned Model configuration\n",
    "finetuned_model_name = config[\"finetuned_model_name\"]\n",
    "finetuned_model_version = config[\"finetuned_model_version\"]\n",
    "\n",
    "# Inference environment configuration\n",
    "inference_env_name = config[\"inference_env_name\"]\n",
    "inference_env_version = config[\"inference_env_version\"]\n",
    "inference_env_conda_file = config[\"inference_env_conda_file\"]\n",
    "inference_env_base_image = config[\"inference_env_base_image\"]\n",
    "\n",
    "# Finetuned model batch endpoint configuration\n",
    "batch_endpoint_name = config[\"batch_endpoint_name\"]\n",
    "batch_deployment_name = config[\"batch_deployment_name\"]\n",
    "batch_cluster_name = config[\"batch_cluster_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb236cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Client initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize ML Client\n",
    "ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)\n",
    "print(\"ML Client initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c27e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the registered model\n",
    "registered_model = ml_client.models.get(name=finetuned_model_name, version=finetuned_model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d1e66cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using existing environment: finetuned-phi4-model-env:4\n"
     ]
    }
   ],
   "source": [
    "# Create or get inference environment\n",
    "try:\n",
    "    # Try to get existing environment\n",
    "    env_asset = ml_client.environments.get(name=inference_env_name, version=inference_env_version)\n",
    "    print(f\"Using existing environment: {inference_env_name}:{inference_env_version}\")\n",
    "except:\n",
    "    # Create new environment if it doesn't exist\n",
    "    print(f\"Creating new environment: {inference_env_name}\")\n",
    "    env_asset = Environment(\n",
    "        name=inference_env_name,\n",
    "        conda_file=inference_env_conda_file,\n",
    "        image=inference_env_base_image,\n",
    "        description=\"Environment for batch inference with fine-tuned model\"\n",
    "    )\n",
    "    env_asset = ml_client.environments.create_or_update(env_asset)\n",
    "    print(f\"Environment created: {env_asset.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ab20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the compute -  I have done this via the Azure ML Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15a7f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new batch endpoint: b-ft-phi4-mini-instruct-endpoint\n",
      "Batch endpoint 'b-ft-phi4-mini-instruct-endpoint' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create Batch Endpoint\n",
    "try:\n",
    "    # Try to get existing endpoint\n",
    "    batch_endpoint = ml_client.batch_endpoints.get(batch_endpoint_name)\n",
    "    print(f\"Using existing batch endpoint: {batch_endpoint_name}\")\n",
    "except:\n",
    "    print(f\"Creating new batch endpoint: {batch_endpoint_name}\")\n",
    "    \n",
    "    # Create new batch endpoint\n",
    "    batch_endpoint = BatchEndpoint(\n",
    "        name=batch_endpoint_name,\n",
    "        description=\"Batch endpoint for fine-tuned model inference\"\n",
    "    )\n",
    "    batch_endpoint = ml_client.batch_endpoints.begin_create_or_update(batch_endpoint).result()\n",
    "    print(f\"Batch endpoint '{batch_endpoint_name}' created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3afa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new batch deployment: green\n",
      "Creating batch deployment... This may take several minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\firozshaik\\AppData\\Roaming\\Python\\Python312\\site-packages\\azure\\ai\\ml\\entities\\_deployment\\batch_deployment.py:137: UserWarning: This class is intended as a base class and it's direct usage is deprecated. Use one of the concrete implementations instead:\n",
      "* ModelBatchDeployment - For model-based batch deployments\n",
      "* PipelineComponentBatchDeployment - For pipeline component-based batch deployments\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch deployment 'green' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create Batch Deployment\n",
    "try:\n",
    "    # Try to get existing deployment\n",
    "    batch_deployment = ml_client.batch_deployments.get(batch_deployment_name, batch_endpoint_name)\n",
    "    print(f\"Using existing batch deployment: {batch_deployment_name}\")\n",
    "except:\n",
    "    print(f\"Creating new batch deployment: {batch_deployment_name}\")\n",
    "    \n",
    "    batch_deployment = BatchDeployment(\n",
    "        name=batch_deployment_name,\n",
    "        endpoint_name=batch_endpoint_name,\n",
    "        model=registered_model.id,\n",
    "        environment=env_asset.id,\n",
    "        code_configuration=CodeConfiguration(\n",
    "            code=\"./serve\", \n",
    "            scoring_script=\"score_batch.py\"\n",
    "        ),\n",
    "        # Use the dedicated compute cluster\n",
    "        compute=batch_cluster_name,\n",
    "        \n",
    "        # Resource configuration for batch processing\n",
    "        resources=ResourceConfiguration(\n",
    "            instance_count=1,  # Number of nodes for parallel processing\n",
    "        ),\n",
    "        \n",
    "        # Retry settings for handling low-priority VM interruptions\n",
    "        retry_settings=BatchRetrySettings(\n",
    "            max_retries=3,  # Retry up to 3 times if interrupted\n",
    "            timeout=300  # 5 minutes timeout per batch\n",
    "        ),\n",
    "        \n",
    "        # Batch-specific settings\n",
    "        max_concurrency_per_instance=1,\n",
    "        mini_batch_size=5,  # Process 5 items per mini-batch\n",
    "        logging_level=\"info\",\n",
    "        description=\"Batch deployment\"\n",
    "    )\n",
    "    \n",
    "    print(\"Creating batch deployment... This may take several minutes.\")\n",
    "    batch_deployment = ml_client.batch_deployments.begin_create_or_update(batch_deployment).result()\n",
    "    print(f\"Batch deployment '{batch_deployment_name}' created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8cf1e843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 'green' as default deployment for batch endpoint\n"
     ]
    }
   ],
   "source": [
    "# Set default deployment for the batch endpoint\n",
    "batch_endpoint.defaults.deployment_name = batch_deployment_name\n",
    "ml_client.batch_endpoints.begin_create_or_update(batch_endpoint).result()\n",
    "print(f\"Set '{batch_deployment_name}' as default deployment for batch endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4336fa",
   "metadata": {},
   "source": [
    "# Testing the batch end point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9a7f464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1273 test samples\n",
      "Sample item keys: ['question', 'answer', 'options', 'meta_info', 'answer_idx']\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "with open(\"data/test.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test samples\")\n",
    "print(f\"Sample item keys: {list(test_data[0].keys()) if test_data else 'No data'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958d3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created batch input file 'batch_input.jsonl' with 20 items\n"
     ]
    }
   ],
   "source": [
    "# Create batch input data for scoring\n",
    "output_file = \"batch_input.jsonl\"\n",
    "max_items = 20\n",
    "batch_data = []\n",
    "for i, item in enumerate(test_data[:max_items]):\n",
    "    question = item[\"question\"]\n",
    "    options = item[\"options\"]\n",
    "    answer_idx = item[\"answer_idx\"]\n",
    "    \n",
    "    # Format options as A. Option text...\n",
    "    formatted_options = \"\\n\".join([f\"{key}. {val}\" for key, val in sorted(options.items())])\n",
    "    \n",
    "    # Create request in the format expected by the model\n",
    "    request = {\n",
    "        \"id\": f\"item_{i}\",  # Unique identifier for tracking\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a medical expert. Read the following USMLE question and choose the best answer. Give me the answer as A/B/C/D/E.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question:\\n{question}\\n\\nOptions:\\n{formatted_options}\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 10,\n",
    "        \"temperature\": 0.1,\n",
    "        \"ground_truth\": answer_idx  # For evaluation purposes\n",
    "    }\n",
    "    batch_data.append(request)\n",
    "\n",
    "# Save to JSONL format for batch processing\n",
    "with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "    for item in batch_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Created batch input file '{output_file}' with {len(batch_data)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754babc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking batch endpoint...\n",
      "Batch job submitted successfully!\n",
      "Job name: batchjob-95796ea7-142b-47b6-bdcc-e9bfeeead3e4\n"
     ]
    }
   ],
   "source": [
    "# Submit batch scoring job\n",
    "input_file = \"batch_input.jsonl\"\n",
    "try:\n",
    "    # Use direct file path approach\n",
    "    input_data = Input(\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        path=f\"./{input_file}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Invoking batch endpoint...\")\n",
    "    job = ml_client.batch_endpoints.invoke(\n",
    "        endpoint_name=batch_endpoint_name,\n",
    "        input=input_data,\n",
    "        deployment_name=batch_deployment_name\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch job submitted successfully!\")\n",
    "    print(f\"Job name: {job.name}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Batch scoring job failed: {e}\")\n",
    "    print(f\"Error details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb25af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifact azureml://datastores/workspaceblobstore/paths/azureml/addd61c0-bb32-4580-b793-e8abd5b27e29/score/ to batch_results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results downloaded to './batch_results'\n"
     ]
    }
   ],
   "source": [
    "# Download the results\n",
    "ml_client.jobs.download(job.name, download_path=\"./batch_results\")\n",
    "print(\"Results downloaded to './batch_results'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "21bbb22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch scoring results:\n",
      "       id prediction full_response ground_truth  prompt_length  \\\n",
      "0  item_0          B             B            C            219   \n",
      "1  item_1          C             C            E            169   \n",
      "2  item_2          C             C            C            338   \n",
      "3  item_3          E             E            D            349   \n",
      "4  item_4          B             B            B            233   \n",
      "\n",
      "   response_length  \n",
      "0                3  \n",
      "1                3  \n",
      "2                3  \n",
      "3                3  \n",
      "4                3  \n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "results_file = os.path.join(\"batch_results\", \"batch_input_results.json\")\n",
    "# read the json results\n",
    "with open(results_file, \"r\", encoding='utf-8') as f:\n",
    "    results = json.load(f)\n",
    "# Display the results\n",
    "if results:\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"Batch scoring results:\")\n",
    "    print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284752ef",
   "metadata": {},
   "source": [
    "### Make sure to delete the clusters (dedicated ones) and endpoints after evaluation. Low-priority clusters scale to 0 automatically, so they don't incur costs when idle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
