{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802d4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.constants import AssetTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2bad6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global logging level\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# Specifically reduce Azure-related logging in this notebook\n",
    "logging.getLogger(\"azure\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"azure.identity\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "143eb4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the Azure ML workspace configuration from config.yml\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Azure ML workspace configuration\n",
    "subscription_id = config[\"subscription_id\"]\n",
    "resource_group = config[\"resource_group\"]\n",
    "workspace_name = config[\"workspace_name\"]\n",
    "\n",
    "# Finetuned model batch endpoint configuration\n",
    "batch_endpoint_name = config[\"batch_endpoint_name\"]\n",
    "batch_deployment_name = config[\"batch_deployment_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82df7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to log in to Azure\n",
    "#!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f593c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the MLClient instance\n",
    "ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a12c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test data\n",
    "with open(\"./data/test.jsonl\", \"r\", encoding='utf-8') as f:\n",
    "    test_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32291f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created batch input file 'batch_input.jsonl' with 1273 items\n"
     ]
    }
   ],
   "source": [
    "# Create batch input data for scoring\n",
    "output_file = \"batch_input.jsonl\"\n",
    "max_items = len(test_data)  # Use all items in the test set\n",
    "batch_data = []\n",
    "for i, item in enumerate(test_data[:max_items]):\n",
    "    question = item[\"question\"]\n",
    "    options = item[\"options\"]\n",
    "    answer_idx = item[\"answer_idx\"]\n",
    "    \n",
    "    # Format options as A. Option text...\n",
    "    formatted_options = \"\\n\".join([f\"{key}. {val}\" for key, val in sorted(options.items())])\n",
    "    \n",
    "    # Create request in the format expected by the model\n",
    "    request = {\n",
    "        \"id\": f\"item_{i}\",  # Unique identifier for tracking\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a medical expert. Read the following USMLE question and choose the best answer. Give me the answer as A/B/C/D/E.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Question:\\n{question}\\n\\nOptions:\\n{formatted_options}\"\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 10,\n",
    "        \"temperature\": 0.1,\n",
    "        \"ground_truth\": answer_idx  # For evaluation purposes\n",
    "    }\n",
    "    batch_data.append(request)\n",
    "\n",
    "# Save to JSONL format for batch processing\n",
    "with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "    for item in batch_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Created batch input file '{output_file}' with {len(batch_data)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c087cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking batch endpoint...\n",
      "Batch job submitted successfully!\n",
      "Job name: batchjob-746f237c-7c5b-4550-b608-62494432be65\n",
      "Batch job submitted successfully!\n",
      "Job name: batchjob-746f237c-7c5b-4550-b608-62494432be65\n"
     ]
    }
   ],
   "source": [
    "# Submit batch scoring job\n",
    "input_file = \"batch_input.jsonl\"\n",
    "try:\n",
    "    # Use direct file path approach\n",
    "    input_data = Input(\n",
    "        type=AssetTypes.URI_FILE,\n",
    "        path=f\"./{input_file}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Invoking batch endpoint...\")\n",
    "    job = ml_client.batch_endpoints.invoke(\n",
    "        endpoint_name=batch_endpoint_name,\n",
    "        input=input_data,\n",
    "        deployment_name=batch_deployment_name\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch job submitted successfully!\")\n",
    "    print(f\"Job name: {job.name}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Batch scoring job failed: {e}\")\n",
    "    print(f\"Error details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6c5959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to resume a batch scoring job if it was interrupted\n",
    "# Note: This requires the job ID from a previous run\n",
    "# input_file = \"batch_input.jsonl\"\n",
    "# try:\n",
    "#     # Use direct file path approach\n",
    "#     input_data = Input(\n",
    "#         type=AssetTypes.URI_FILE,\n",
    "#         path=f\"./{input_file}\"\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Invoking batch endpoint...\")\n",
    "#     job = ml_client.batch_endpoints.invoke(\n",
    "#         endpoint_name=batch_endpoint_name,\n",
    "#         input=input_data,\n",
    "#         deployment_name=batch_deployment_name,\n",
    "#         resume_from=\"batchjob-29ca7dc5-aa57-4289-935e-f03b0428891c\"\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Batch job resubmitted successfully!\")\n",
    "#     print(f\"Job name: {job.name}\")\n",
    "\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Batch scoring job failed: {e}\")\n",
    "#     print(f\"Error details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d49cbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifact azureml://datastores/workspaceblobstore/paths/azureml/f5f75641-b175-494f-87f6-9c5519cde1fc/score/ to batch_results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results downloaded to './batch_results'\n"
     ]
    }
   ],
   "source": [
    "# Download the results\n",
    "ml_client.jobs.download(job.name, download_path=\"./batch_results\")\n",
    "print(\"Results downloaded to './batch_results'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e70f1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of predictions: 1273\n",
      "\n",
      "Batch Inference Results:\n",
      "Correct predictions: 592\n",
      "Total predictions: 1273\n",
      "Accuracy: 46.50%\n"
     ]
    }
   ],
   "source": [
    "# Finetuned model batch endpoint evaluation\n",
    "with open(\"./batch_results/batch_input_results.json\", \"r\", encoding='utf-8') as f:\n",
    "    batch_results = json.load(f)\n",
    "\n",
    "print(f\"Total number of predictions: {len(batch_results)}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = 0\n",
    "total_predictions = len(batch_results)\n",
    "\n",
    "for result in batch_results:\n",
    "    prediction = result[\"prediction\"]\n",
    "    ground_truth = result[\"ground_truth\"]\n",
    "    \n",
    "    if prediction == ground_truth:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "print(f\"\\nBatch Inference Results:\")\n",
    "print(f\"Correct predictions: {correct_predictions}\")\n",
    "print(f\"Total predictions: {total_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a4003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
